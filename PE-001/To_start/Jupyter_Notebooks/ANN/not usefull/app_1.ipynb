{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ad3ed62a2b61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;31m#returns a numpy array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m#scaler = preprocessing.MinMaxScaler()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mx_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessing' is not defined"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import sys\n",
    "import datetime, os\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "column_names = ['sgs_p','rho','tempe','press','rhoe',\n",
    "                'gradrho', 'gradt', 'gradp','gradroe','laprho','laprhoe','inv1','inv2','inv3']\n",
    "raw_dataset = pd.read_csv('data.dat', names=column_names,\n",
    "                      sep=\" \",nrows = 10000)\n",
    "\n",
    "data = raw_dataset.copy()\n",
    "\n",
    "data.columns = ['sgs_p','rho','tempe','press','rhoe',\n",
    "                'gradrho', 'gradt', 'gradp','gradroe','laprho','laprhoe','inv1','inv2','inv3']\n",
    "\n",
    "x = data.values #returns a numpy array\n",
    "#scaler = preprocessing.MinMaxScaler()\n",
    "scaler = preprocessing.StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "                          \n",
    "data = pd.DataFrame(x_scaled)\n",
    "data.columns = ['sgs_p','rho','tempe','press','rhoe',\n",
    "                'gradrho', 'gradt', 'gradp','gradroe','laprho','laprhoe','inv1','inv2','inv3']\n",
    "\n",
    "train_dataset = data.sample(frac=0.75,random_state=0)\n",
    "test_dataset = data.drop(train_dataset.index)\n",
    "\n",
    "train_labels = train_dataset.pop('sgs_p')\n",
    "test_labels = test_dataset.pop('sgs_p')\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "train_dataset = np.asarray(train_dataset)\n",
    "test_dataset = np.asarray(test_dataset)\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "    model = keras.Sequential([\n",
    "    layers.Dense(10, use_bias=True,activation='relu', input_shape=[train_dataset.shape[1]]),    \n",
    "    layers.Dense(10, use_bias=True,activation='relu'),     \n",
    "    layers.Dense(1, use_bias=True,activation='linear'),\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n",
    "\n",
    "def train_model():\n",
    "  \n",
    "    \n",
    "  optimizer = tf.optimizers.Adam() #\n",
    "\n",
    "  model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mse'])\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "  early_stop = keras.callbacks.EarlyStopping(monitor='val_mse',min_delta=1e-17, patience=10)\n",
    "  logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1,profile_batch=0)\n",
    "\n",
    "\n",
    "  model.fit(train_dataset, \n",
    "            train_labels, \n",
    "            epochs=100,\n",
    "            batch_size=1000,\n",
    "            validation_split = 0.5, \n",
    "            verbose=0, \n",
    "            callbacks=[early_stop,tensorboard_callback])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
